experiment:
  generate_experiment_dir: test/experiments
  experiment_name: testing # prefix
  experiment_type: sweep # standard, seeded_runs, sweep
  benchmark_dir: tests/data/geobench-segmentation-test
  partition_name: default
  loggers: ["csv"]
model:
  model_generator_module_name: geobench_exp.torch_toolbox.model_generators.py_segmentation_generator
  head_type: linear # classification
  loss_type: crossentropy
  new_channel_init_method: random
  encoder_type: resnet18  # resnet18, convnext_base
  decoder_type: Unet
  encoder_weights: imagenet
  desired_input_size: 224 # desired image input dimension to model
  pretrained: False 
  lr_backbone: 1.0e-3
  lr_head: 1.0e-2
  optimizer: sgd
  hidden_size: 512
  early_stopping_metric: train_loss
dataset:
  band_names: ["red", "green", "blue"]
  format: hdf5
dataloader:
  num_workers: 0
  batch_size: 2
wandb:
  project: geobench_exp # wandb project name
  entity: climate-benchmark # user or team entity that hosts a project
  mode: offline
  sweep:
    sweep_config_path: tests/configs/sweep_config.yaml # or maybe define the entire sweep here?
    num_agents: 4 # how many agents participate in sweep, one agent usually corresponds to one gpu
    num_trials_per_agent: 5 # how many hparam trials each agent should execute, num_agents x num_trials_per_agent corresponds to total number of trials executed by sweep
pl: # all flags for pytorch lightning Trainer module, see https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html
  accelerator: cpu
  max_epochs: 1
  max_steps: -1
  limit_val_batches: 1.0
  limit_test_batches: 1.0
  val_check_interval: 1.0
  deterministic: False
  log_every_n_steps: 1
  enable_progress_bar: False